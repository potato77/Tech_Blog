### 强化学习学习手册



基本概念

机器学习分为：监督学习，无监督学习，半监督学习（也可以用hinton所说的强化学习）等。

**监督学习**（supervised learning）：使用已知正确答案的示例来训练网络。

在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）

有监督学习是从外部监督者提供的带标注训练集进行学习，所谓标注，即针对当前情境，系统应做出的正确动作。采用这种学习方式是为了让系统能够具备推断或泛化能力，能够响应不同的情境并做出正确的动作，哪怕这个情境没有在训练集合中出现过。这种学习方式不适合交互式学习，不可能获得在所有情况下既正确又有代表性的动作。



**无监督学习**：

在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。

无监督学习是一个典型的寻找未标注数据中隐含结构的过程。强化学习尽管也是无标注样本学习，但是其目的是最大化收益信号，而不是找出数据的隐含结构。



**半监督学习**：


在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。

 

**强化学习**：


在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）



学习者不会被告知应该采取什么动作，而是必须自己通过尝试去发现哪些动作会产生最丰厚的收益，同时，动作往往影响的不仅仅是即时收益，也会影响下一个情境，从而影响随后的收益。因此，**试错**和**延时收益**是强化学习两个最重要最显著的特征。



强化学习（Reinforcement Learning，又称为增强学习）这一名词来源于行为心理学，表示生物为了趋利避害而更加频繁实施对自己有利的策略。



强化学习的关键元素：

- 奖励（reward）
- 策略（policy）



**智能体**（agent）是强化学习系统中的决策者和学习者，它可以做出决策和接受奖励信号。

**环境**（environment）是强化学习系统中除智能体以外的所有事物，它是智能体交互的对象。



除了智能体和环境之外，强化学习系统有四个核心要素：**策略**、**收益信号**、**价值函数**和**环境模型**（可选）。

**策略**定义了学习智能体在特定时间的行为方式。简单地说，策略是环境状态到动作的映射，某些情况下，策略可以是一个简单的函数或者查询表，而另一些情况，策略可能涉及大量的运算。

**收益信号**定义了强化学习问题的目标。在每一步中，环境向强化学习智能体发送一个称为收益的标量数值。智能体的唯一目标是最大化长期总收益。

收益信号表明了在短时间内什么是好的，而**价值函数**则表示了从长远角度看什么是好的。一个状态的价值是一个智能体从当前状态开始，对将来累积的总收益的期望。











强化学习的 2 种学习/优化方案：

- 基于价值 Value-based （每一步 State 给奖励）—— 最终 Agent 获得每一步最优解（确定性策略）
  - Sarsa
  - Q-learning
  - DQN
- 基于策略 Policy-based （最终给出奖励）—— 最终 Agent 获得每一步的概率分布（随机性策略）
  - Policy gradient
    

强化学习算法分类：

- 基于模型 Model-based
  - 动态规划
- 无模型 Model-free
  - 基于价值 Value-based
    - on-Policy：Sarsa
    - off-Policy：Q-learning，DQN
  - 基于策略 Policy-based
    - Policy Gradient
      - Actor-Critic
        - DDPG，A3C	
      - TRPO
        - PPO
          

![image-20210423103302410](C:\Users\WINDOWS ROG\Documents\Tech_Blog\科研笔记\强化学习.assets\image-20210423103302410.png)



GYM库（环境库）

PARL库（算法库）



强化学习MDP四元组<S,A,P,R>

智能体观测环境，可以获得环境的观测（observation），记为O；

智能体根据观测做出决策，决定要对环境施加的动作（action），记为A；

环境受智能体动作的影响，改变自己的状态（state），记为S，并给出奖励（reward），记为R







#### 深度强化学习 Deep reinforcement learning

深度强化学习将[深度学习](https://baike.baidu.com/item/深度学习/3729729)的感知能力和[强化学习](https://baike.baidu.com/item/强化学习/2971075)的决策能力相结合，可以直接根据输入的图像进行控制，是一种更接近人类思维方式的人工智能方法。



##### DQN算法 谷歌2015年提出

DQN算法融合了神经网络和Q learning的方法， 名字叫做 Deep Q Network。

DQN 有一个记忆库用于学习之前的经历。在之前的简介影片中提到过， Q learning 是一种 off-policy 离线学习法， 它能学习当前经历着的， 也能学习过去经历过的， 甚至是学习别人的经历. 所以每次 DQN 更新的时候， 我们都可以随机抽取一些之前的经历进行学习. 随机抽取这种做法打乱了经历之间的相关性， 也使得神经网络更新更有效率。Fixed Q-targets 也是一种打乱相关性的机理， 如果使用 fixed Q-targets， 我们就会在 DQN 中使用到两个结构相同但参数不同的神经网络， 预测 Q 估计 的神经网络具备最新的参数， 而预测 Q 现实 的神经网络使用的参数则是很久以前的。有了这两种提升手段， DQN 才能在一些游戏中超越人类。



参考文章：https://www.cnblogs.com/xiaohuiduan/p/12945449.html

DQN 入门教程：https://www.cnblogs.com/xiaohuiduan/category/1770037.html

![image-20210607105739778](https://gitee.com/potato77/pic/raw/master/img/20210607105739.png)

![image-20210607105805015](https://gitee.com/potato77/pic/raw/master/img/20210607105805.png)

**DQN中的Q函数就是价值函数**

![image-20210607105847737](https://gitee.com/potato77/pic/raw/master/img/20210607105847.png)

![image-20210607105934612](https://gitee.com/potato77/pic/raw/master/img/20210607105934.png)

![image-20210607110006955](https://gitee.com/potato77/pic/raw/master/img/20210607110006.png)

![image-20210607110808336](https://gitee.com/potato77/pic/raw/master/img/20210607110808.png)

![image-20210607110956772](https://gitee.com/potato77/pic/raw/master/img/20210607110956.png)

**Q 函数称为从状态 s 开始，使用 a 作为第一个行为的最大累积奖励值，又称作价值函数（value-function）- 动作价值函数**

![image-20210607111217117](https://gitee.com/potato77/pic/raw/master/img/20210607111217.png)

![image-20210607113130851](https://gitee.com/potato77/pic/raw/master/img/20210607113130.png)

![image-20210607113333102](https://gitee.com/potato77/pic/raw/master/img/20210607113333.png)



![image-20210607113629626](https://gitee.com/potato77/pic/raw/master/img/20210607113629.png)

![image-20210607113724375](https://gitee.com/potato77/pic/raw/master/img/20210607113724.png)

Q-learning是离散的

在传统的DNN or CNN网络中，我们是已知训练集，然后进**多次训练**的。但是在强化学习中，训练集是未知的，因为我们的要求是机器进行自我学习。换句话来说，就是神经网络的更新是实时的，一边进行游戏得到数据集一边使用数据进行训练。

![image-20210607114201651](https://gitee.com/potato77/pic/raw/master/img/20210607114201.png)

![img](https://img2020.cnblogs.com/blog/1439869/202005/1439869-20200530170346819-692427301.png)





## DDPG

![image-20210607115006332](https://gitee.com/potato77/pic/raw/master/img/20210607115006.png)

![image-20210607115049097](https://gitee.com/potato77/pic/raw/master/img/20210607115049.png)

![image-20210607115114010](https://gitee.com/potato77/pic/raw/master/img/20210607115114.png)

![img](https://img-blog.csdnimg.cn/img_convert/7e9d8a9fb6b045b01d6dc4a25ef409eb.png)